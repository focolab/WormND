{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import os\n",
    "\n",
    "from adjustText import adjust_text\n",
    "import colorsys\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from hdmf.backends.hdf5.h5_utils import H5DataIO\n",
    "from hdmf.container import Container\n",
    "from hdmf.data_utils import DataChunkIterator\n",
    "import latex\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pynwb import load_namespaces, get_class, register_class, NWBFile, TimeSeries, NWBHDF5IO\n",
    "from pynwb.file import MultiContainerInterface, NWBContainer, Device, Subject\n",
    "from pynwb.ophys import ImageSeries, OnePhotonSeries, OpticalChannel, ImageSegmentation, PlaneSegmentation, Fluorescence, DfOverF, CorrectedImageStack, MotionCorrection, RoiResponseSeries, ImagingPlane\n",
    "from pynwb.core import NWBDataInterface\n",
    "from pynwb.epoch import TimeIntervals\n",
    "from pynwb.behavior import SpatialSeries, Position\n",
    "from pynwb.image import ImageSeries\n",
    "import pywt\n",
    "import scipy.io as sio\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal, spearmanr\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import skimage.io as skio\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from tifffile import TiffFile\n",
    "import tifffile\n",
    "\n",
    "from networkx import kamada_kawai_layout\n",
    "\n",
    "import sys\n",
    "sys.path.append('NWBelegans/Analysis')\n",
    "\n",
    "from atlas import loadmat, NPAtlas, NWBAtlas\n",
    "from process_file import get_nwb_neurons, get_dataset_neurons, get_dataset_online, combine_datasets, get_pairings, get_color_discrim, get_neur_nums\n",
    "from stats import get_summary_stats, analyze_pairs, get_accuracy\n",
    "from visualization import plot_num_heatmap, plot_std_heatmap, plot_summary_stats, plot_color_discrim, plot_accuracies, plot_visualizations_atlas, plot_visualizations_data, plot_atlas2d_super\n",
    "from utils import covar_to_coord, convert_coordinates, maha_dist, run_linear_assignment\n",
    "\n",
    "# ndx_mulitchannel_volume is the novel NWB extension for multichannel optophysiology in C. elegans\n",
    "#from ndx_multichannel_volume import CElegansSubject, OpticalChannelReferences, OpticalChannelPlus, ImagingVolume, VolumeSegmentation, MultiChannelVolume, MultiChannelVolumeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional import if you want to open figures in a separate window, add %matplotlib qt to top of any code box if you want figures to open in a separate window \n",
    "import PyQt6.QtCore\n",
    "os.environ[\"QT_API\"] = \"pyqt6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = NWBAtlas(atlas_file = 'Data/atlases/2024_03_11_match_full_nosplit.pkl', ganglia='Data/neuron_ganglia.csv') # Load atlas\n",
    "atlas_df = atlas.get_df()\n",
    "atlas_neurons = np.asarray(atlas_df['ID'])\n",
    "atlas.df = atlas.df.drop(atlas.df[atlas.df['ID']=='IL1V'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maha_dist(data, mu, sigma):\n",
    "\n",
    "    data_mu = data-mu\n",
    "    inv_sigma = np.linalg.inv(sigma)\n",
    "    left_data = np.dot(data_mu, inv_sigma)\n",
    "    mahal = np.dot(left_data, data_mu.T)\n",
    "\n",
    "    return np.sqrt(mahal)\n",
    "\n",
    "def run_linear_assignment(df_data, atlas):\n",
    "    #df_data should have columns 'X', 'Y', 'Z', 'R', 'G', 'B', 'ID' \n",
    "\n",
    "    df_assigns = df_data.copy()\n",
    "\n",
    "    mu = atlas.mu\n",
    "    sigma = atlas.sigma\n",
    "    neurons = np.asarray(atlas.neurons)\n",
    "\n",
    "    xyzrgb = np.asarray(df_data[['X','Y','Z','R','G','B']])\n",
    "    gt_labels = np.asarray(df_data['ID'])\n",
    "\n",
    "    assigns = np.empty((xyzrgb.shape[0],5),np.dtype('U100'))\n",
    "    assign_cost = np.zeros((xyzrgb.shape[0], 3, 5)) #total, position, color in second dimension, top 5 ranks in third \n",
    "\n",
    "    cost_mat = np.zeros((xyzrgb.shape[0], mu.shape[0]))\n",
    "    cost_pos = np.zeros((xyzrgb.shape[0], mu.shape[0]))\n",
    "    cost_col = np.zeros((xyzrgb.shape[0], mu.shape[0]))\n",
    "\n",
    "    for i in range(xyzrgb.shape[0]):\n",
    "        for j in range(mu.shape[0]):\n",
    "            cost = maha_dist(xyzrgb[i,:], mu[j,:], sigma[:,:,j])\n",
    "            cost_pos[i,j] = maha_dist(xyzrgb[i,:3], mu[j,:3], sigma[:3,:3,j])\n",
    "            cost_col[i,j] = maha_dist(xyzrgb[i,3:], mu[j,3:], sigma[3:,3:,j])\n",
    "\n",
    "            cost_mat[i,j] = cost\n",
    "\n",
    "    for k in range(5):\n",
    "\n",
    "        row_inds, col_inds = linear_sum_assignment(cost_mat)\n",
    "\n",
    "        assigns[row_inds,k] = np.asarray(neurons[col_inds])\n",
    "\n",
    "        assign_cost[row_inds, 0, k] = cost_mat[row_inds, col_inds]\n",
    "        assign_cost[row_inds, 1, k] = cost_pos[row_inds, col_inds]\n",
    "        assign_cost[row_inds, 2, k] = cost_col[row_inds, col_inds]\n",
    "\n",
    "        cost_mat[row_inds, col_inds] = np.inf\n",
    "\n",
    "    df_assigns['assign_1'] = assigns[:,0]\n",
    "    df_assigns['assign_2'] = assigns[:,1]\n",
    "    df_assigns['assign_3'] = assigns[:,2]\n",
    "    df_assigns['assign_4'] = assigns[:,3]\n",
    "    df_assigns['assign_5'] = assigns[:,4]\n",
    "\n",
    "    return df_assigns, assign_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(folder, atlas):\n",
    "    acc_df = pd.DataFrame(columns=['Total_neurons','Percent_IDd', 'Percent_top1', 'Percent_top2', 'Percent_top3', 'Percent_top4', 'Percent_top5', 'Filename'])\n",
    "    for file in os.listdir(folder):\n",
    "        if not file[-4:] == '.csv':\n",
    "            continue\n",
    "\n",
    "        df_data = pd.read_csv(folder + '/'+file)\n",
    "        df_data = df_data.rename(columns={\"aligned_x\":\"X\",\"aligned_y\":\"Y\",\"aligned_z\":\"Z\", \"aligned_R\":\"R\", \"aligned_G\":\"G\", \"aligned_B\":\"B\"})\n",
    "\n",
    "        df, costs = run_linear_assignment(df_data, atlas)\n",
    "\n",
    "        IDd = df[~df['ID'].isnull()]\n",
    "\n",
    "        per_ID = len(IDd.index)/len(df.index)\n",
    "\n",
    "        total_neurons = len(df.index)\n",
    "\n",
    "        corr1 = df.loc[df['ID']==df['assign_1']]\n",
    "        corr2 = df.loc[df['ID']==df['assign_2']]\n",
    "        corr3 = df.loc[df['ID']==df['assign_3']]\n",
    "        corr4 = df.loc[df['ID']==df['assign_4']]\n",
    "        corr5 = df.loc[df['ID']==df['assign_5']]\n",
    "            \n",
    "        corr_cum_2 = pd.concat([corr1,corr2]).drop_duplicates().reset_index(drop=True)\n",
    "        corr_cum_3 = pd.concat([corr_cum_2,corr3]).drop_duplicates().reset_index(drop=True)\n",
    "        corr_cum_4 = pd.concat([corr_cum_3,corr4]).drop_duplicates().reset_index(drop=True)\n",
    "        corr_cum_5 = pd.concat([corr_cum_4, corr5]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        per_corr_1 = len(corr1.index)/len(IDd.index)\n",
    "        per_corr_2 = len(corr_cum_2.index)/len(IDd.index)\n",
    "        per_corr_3 = len(corr_cum_3.index)/len(IDd.index)\n",
    "        per_corr_4 = len(corr_cum_4.index)/len(IDd.index)\n",
    "        per_corr_5 = len(corr_cum_5.index)/len(IDd.index)\n",
    "\n",
    "        acc_df.loc[len(acc_df.index)] = [total_neurons,per_ID, per_corr_1, per_corr_2, per_corr_3, per_corr_4, per_corr_5, file[:-4]]\n",
    "\n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get accuracy values for each dataset using the trained atlas and the roughly aligned point clouds. If you would like\n",
    "to test on datasets that have not been pre-aligned, please use the neuroPAL_ID software which has the atlas and alignment\n",
    "code pre-compiled\n",
    "'''\n",
    "\n",
    "NP_atlas_match = NWBAtlas(atlas_file = 'Data/atlases/2024_03_11_NPonly.pkl', ganglia='Data/neuron_ganglia.csv') #Atlas trained on just original 10 NeuroPAL datasets\n",
    "NP_atlas_unmatch = NWBAtlas(atlas_file = 'Data/atlases/2024_03_11_NPunmatch.pkl', ganglia='Data/neuron_ganglia.csv')\n",
    "\n",
    "accs_NP = get_accuracies('Data/aligned_heads/aligned_NP', NP_atlas_match)\n",
    "accs_NP_unmatch = get_accuracies('Data/aligned_heads/aligned_NP_nomatch', NP_atlas_unmatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    full_atlas_match = NWBAtlas(atlas_file = 'Data/atlases/2024_03_11_split/exgroup'+str(i)+'.pkl', ganglia='Data/neuron_ganglia.csv') \n",
    "    full_atlas_unmatch = NWBAtlas(atlas_file = 'Data/atlases/2024_03_11_split_unmatch/exgroup'+str(i)+'.pkl', ganglia='Data/neuron_ganglia.csv')\n",
    "\n",
    "    accs_match = get_accuracies('Data/aligned_heads/aligned_split/group'+str(i+1), full_atlas_match)\n",
    "    accs_unmatch = get_accuracies('Data/aligned_heads/aligned_split_nomatch/group'+str(i+1), full_atlas_unmatch)\n",
    "\n",
    "    if i==0:\n",
    "        accs_full = accs_match\n",
    "        accs_full_unmatch = accs_unmatch\n",
    "    else:\n",
    "        accs_full = pd.concat([accs_full, accs_match])\n",
    "        accs_full_unmatch = pd.concat([accs_full_unmatch, accs_unmatch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_NP_unmatch_CRF = pd.read_csv('Data/Acc_CRFID/original.csv')\n",
    "accs_full_CRF = pd.read_csv('Data/Acc_CRFID/multi_colorcorr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_full_CPD = pd.read_csv('Data/Acc_CPD/match_all_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_pretrain_fDNC = pd.read_csv('Data/Acc_fDNC/fDNC_pretrain__acc_top1_and_top5_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping files that have obvious artifacts or known alignment issues\n",
    "skipfiles = ['20231013-9-30-0', '20230412-20-15-17', '2023-01-23-01', '20239828-11-14-0', '2023-01-05-01', '2023-01-10-14', '2022-06-28-07', '2022-07-26-01', '2023-01-19-15', '2022-07-15-06', '2022-08-02-01', '2023-01-09-08', '2023-01-09-28', '2023-01-10-14', '2023-01-17-14', '2023-01-19-22', '2023-01-23-01', '7_YAaLR', '11_YAaLR', '20_YAaLR', '38_YAaDV', '55_YAaDV', '56_YAaDV', '62_YAaLR', '64_YAaDV', '70_YAaLR', '76_YAaDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_assigns = pd.read_csv('Data/group_assigns.csv')\n",
    "dataset_dict = {row['Filename']:row['Dataset'] for i, row in group_assigns.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 33.    53.25  69.   155.5  184.  ]\n",
      "CPD (best template): low labels\n",
      "Rank 1 mean and std\n",
      "0.3850112050363181\n",
      "0.1535105216395126\n",
      "Rank 5 mean and std\n",
      "0.6464095162526102\n",
      "0.14108426302043475\n",
      "CPD (best template): high labels\n",
      "Rank 1 mean and std\n",
      "0.3883926904121189\n",
      "0.08531298227392174\n",
      "Rank 5 mean and std\n",
      "0.6681316874276109\n",
      "0.09713432191677596\n",
      "CPD (best template): laverage\n",
      "Rank 1 mean and std\n",
      "0.3860544292480013\n",
      "0.1361732452938966\n",
      "Rank 5 mean and std\n",
      "0.6531110371470252\n",
      "0.12951995549746173\n",
      "StatAtlas (base): low labels\n",
      "Rank 1 mean and std\n",
      "0.40485059579775506\n",
      "0.1173072176469839\n",
      "Rank 5 mean and std\n",
      "0.6353057706543227\n",
      "0.11803104883514566\n",
      "StatAtlas (base): high labels\n",
      "Rank 1 mean and std\n",
      "0.4100999598332594\n",
      "0.07654098783333474\n",
      "Rank 5 mean and std\n",
      "0.7173536710308535\n",
      "0.05494133851038358\n",
      "StatAtlas (base): laverage\n",
      "Rank 1 mean and std\n",
      "0.4064700804470064\n",
      "0.10643715852570117\n",
      "Rank 5 mean and std\n",
      "0.6606184207704863\n",
      "0.10954788897962951\n",
      "StatAtlas (retrain): low labels\n",
      "Rank 1 mean and std\n",
      "0.6602282675580602\n",
      "0.14035621267140455\n",
      "Rank 5 mean and std\n",
      "0.8897144386967294\n",
      "0.07039554321507492\n",
      "StatAtlas (retrain): high labels\n",
      "Rank 1 mean and std\n",
      "0.5368555992944255\n",
      "0.05891463573182271\n",
      "Rank 5 mean and std\n",
      "0.787573321493269\n",
      "0.05705661735874154\n",
      "StatAtlas (retrain): laverage\n",
      "Rank 1 mean and std\n",
      "0.6221664869235346\n",
      "0.13394085454104596\n",
      "Rank 5 mean and std\n",
      "0.8582028174318319\n",
      "0.08158854841171367\n",
      "CRF (base): low labels\n",
      "Rank 1 mean and std\n",
      "0.5940903507384615\n",
      "0.20268290211873044\n",
      "Rank 5 mean and std\n",
      "0.8428268586461538\n",
      "0.19123729191091257\n",
      "CRF (base): high labels\n",
      "Rank 1 mean and std\n",
      "0.46582513058620695\n",
      "0.06701202092371503\n",
      "Rank 5 mean and std\n",
      "0.7292395127586208\n",
      "0.05349017817261842\n",
      "CRF (base): laverage\n",
      "Rank 1 mean and std\n",
      "0.5545191657978724\n",
      "0.1824876993481736\n",
      "Rank 5 mean and std\n",
      "0.8077839540638297\n",
      "0.1700708043364788\n",
      "CRF (retrain): low labels\n",
      "Rank 1 mean and std\n",
      "0.8129969662307692\n",
      "0.09336975643274564\n",
      "Rank 5 mean and std\n",
      "0.9466077059999999\n",
      "0.05485150960176392\n",
      "CRF (retrain): high labels\n",
      "Rank 1 mean and std\n",
      "0.5712631802068965\n",
      "0.057796580316053656\n",
      "Rank 5 mean and std\n",
      "0.7785603879310345\n",
      "0.03501548389900383\n",
      "CRF (retrain): laverage\n",
      "Rank 1 mean and std\n",
      "0.7384195216063829\n",
      "0.13973188083042445\n",
      "Rank 5 mean and std\n",
      "0.8947633206382978\n",
      "0.0921043182122445\n",
      "fDNC (base): low labels\n",
      "Rank 1 mean and std\n",
      "0.5457543971273588\n",
      "0.1435793390720515\n",
      "Rank 5 mean and std\n",
      "0.7693942783620098\n",
      "0.10320028457863252\n",
      "fDNC (base): high labels\n",
      "Rank 1 mean and std\n",
      "0.2751092826677\n",
      "0.16430331085515257\n",
      "Rank 5 mean and std\n",
      "0.49678656775109753\n",
      "0.19551857973627249\n",
      "fDNC (base): laverage\n",
      "Rank 1 mean and std\n",
      "0.4622575001132088\n",
      "0.19547324629307533\n",
      "Rank 5 mean and std\n",
      "0.6852918995565155\n",
      "0.18711469786743787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/6ffbr8g11tqbyqmft2vx0y5h0000gn/T/ipykernel_81280/286693160.py:104: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  axs[1].set_xticklabels(['', 'Base', 'Retrained', 'Base', 'Retrained'])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "def gen_plots_benchmark(accs_full_CPD, accs_NP_unmatch_stat_atlas, accs_full_stat_atlas, accs_NP_unmatch_CRF_ID, accs_full_CRF_ID, accs_pretrain_fDNC, dataset_dict, skipfiles):\n",
    "\n",
    "    filenames = accs_NP_unmatch_stat_atlas['Filename']\n",
    "\n",
    "    df_dataset = pd.DataFrame(columns = ['Filename', 'Dataset', 'Label','Atlas', 'Model', 'Accuracy', 'Rank', 'Num_labeled'])\n",
    "\n",
    "    df_quartiles = accs_NP_unmatch[~accs_NP_unmatch['Filename'].isin(skipfiles)]\n",
    "    \n",
    "    neur_quartiles = np.quantile(df_quartiles['Total_neurons']*df_quartiles['Percent_IDd'], [0,0.25,0.5,0.75,1])\n",
    "\n",
    "    print(neur_quartiles)\n",
    "\n",
    "    thresh = neur_quartiles[2]\n",
    "    thresh = 100\n",
    "\n",
    "    for file in filenames:\n",
    "        if file in skipfiles:\n",
    "            continue\n",
    "        acc_NP_stat = accs_NP_unmatch_stat_atlas.loc[accs_NP_unmatch_stat_atlas['Filename']==file]\n",
    "        acc_full_stat = accs_full_stat_atlas.loc[accs_full_stat_atlas['Filename']==file]\n",
    "        acc_NP_CRF = accs_NP_unmatch_CRF_ID.loc[accs_NP_unmatch_CRF_ID['Filename']==file]\n",
    "        acc_full_CRF = accs_full_CRF_ID.loc[accs_full_CRF_ID['Filename']==file]\n",
    "        acc_full_CPD = accs_full_CPD.loc[accs_full_CPD['Filename']==file]\n",
    "        acc_pretrain_fDNC = accs_pretrain_fDNC.loc[accs_pretrain_fDNC['Filename']==file]\n",
    "\n",
    "        dataset = dataset_dict[file]\n",
    "\n",
    "        per_IDd = acc_NP_stat.iloc[0]['Percent_IDd']\n",
    "        total_neurons = acc_NP_stat.iloc[0]['Total_neurons']\n",
    "\n",
    "        num_label = per_IDd*total_neurons\n",
    "\n",
    "        for i in range(1,6):\n",
    "            df_dataset.loc[len(df_dataset.index)] = [file, dataset,'CPD (best template)', 'Full','CPD', acc_full_CPD.iloc[0]['top'+str(i)], i, num_label]\n",
    "            df_dataset.loc[len(df_dataset.index)] = [file, dataset,'StatAtlas (base)' ,'Base', 'StatAtlas',  acc_NP_stat.iloc[0]['Percent_top'+str(i)], i, num_label]\n",
    "            df_dataset.loc[len(df_dataset.index)] = [file, dataset,'StatAtlas (retrain)','Full', 'StatAtlas',  acc_full_stat.iloc[0]['Percent_top'+str(i)], i, num_label]\n",
    "            df_dataset.loc[len(df_dataset.index)] = [file, dataset,'CRF (base)','Base', 'CRFID',  acc_NP_CRF.iloc[0]['top'+str(i)], i, num_label]\n",
    "            df_dataset.loc[len(df_dataset.index)] = [file, dataset,'CRF (retrain)','Full', 'CRFID',  acc_full_CRF.iloc[0]['top'+str(i)], i, num_label]\n",
    "            df_dataset.loc[len(df_dataset.index)] = [file, dataset,'fDNC (base)','Base', 'fDNC',  acc_pretrain_fDNC.iloc[0]['top'+str(i)], i, num_label]\n",
    "            \n",
    "    \n",
    "    for label in ['CPD (best template)', 'StatAtlas (base)', 'StatAtlas (retrain)', 'CRF (base)', 'CRF (retrain)', 'fDNC (base)']:\n",
    "\n",
    "        print(label + ': low labels')\n",
    "        print('Rank 1 mean and std')\n",
    "        print(np.mean(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==1)&(df_dataset['Num_labeled']<thresh)]['Accuracy'])))\n",
    "        print(np.std(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==1)&(df_dataset['Num_labeled']<thresh)]['Accuracy'])))\n",
    "        print('Rank 5 mean and std')\n",
    "        print(np.mean(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==5)&(df_dataset['Num_labeled']<thresh)]['Accuracy'])))\n",
    "        print(np.std(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==5)&(df_dataset['Num_labeled']<thresh)]['Accuracy'])))\n",
    "        print(label + ': high labels')\n",
    "        print('Rank 1 mean and std')\n",
    "        print(np.mean(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==1)&(df_dataset['Num_labeled']>=thresh)]['Accuracy'])))\n",
    "        print(np.std(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==1)&(df_dataset['Num_labeled']>=thresh)]['Accuracy'])))\n",
    "        print('Rank 5 mean and std')\n",
    "        print(np.mean(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==5)&(df_dataset['Num_labeled']>=thresh)]['Accuracy'])))\n",
    "        print(np.std(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==5)&(df_dataset['Num_labeled']>=thresh)]['Accuracy'])))\n",
    "        print(label + ': laverage')\n",
    "        print('Rank 1 mean and std')\n",
    "        print(np.mean(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==1)]['Accuracy'])))\n",
    "        print(np.std(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==1)]['Accuracy'])))\n",
    "        print('Rank 5 mean and std')\n",
    "        print(np.mean(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==5)]['Accuracy'])))\n",
    "        print(np.std(np.asarray(df_dataset[(df_dataset['Label']==label)&(df_dataset['Rank']==5)]['Accuracy'])))\n",
    "    \n",
    "    fig, axs = plt.subplots(2,1, sharex=True)\n",
    "\n",
    "    palette = sns.color_palette('colorblind')\n",
    "    color1 = palette[3]\n",
    "    color2 = palette[2]\n",
    "    color3 = palette[0]\n",
    "    color4 = palette[8]\n",
    "    color5 = palette[4]\n",
    "    color6 = palette[6]\n",
    "\n",
    "    sns.violinplot(ax = axs[0], data=df_dataset[df_dataset['Rank']==1], x='Label', y='Accuracy', hue='Model', cut=0, inner='point', density_norm='width', inner_kws = {})\n",
    "    sns.violinplot(ax = axs[1], data=df_dataset[df_dataset['Rank']==5], x='Label', y='Accuracy', hue='Model', cut=0, inner='point', density_norm='width', inner_kws = {})\n",
    "\n",
    "    df_rank = df_dataset[df_dataset['Rank']==1]\n",
    "    df_rank5 = df_dataset[df_dataset['Rank']==5]\n",
    "\n",
    "    stats = df_rank.groupby('Label')['Accuracy'].agg(['mean']).reset_index()\n",
    "    stats5 = df_rank5.groupby('Label')['Accuracy'].agg(['mean']).reset_index()\n",
    "\n",
    "    for i, row in stats.iterrows():\n",
    "        cat_index = np.where(df_rank['Label'].unique() == row['Label'])[0][0]\n",
    "        axs[0].axhline(y=row['mean'], color='red', linestyle='-', linewidth=3, xmin=cat_index/len(df_rank['Label'].unique()), xmax=(cat_index+1)/len(df_rank['Label'].unique()))\n",
    "\n",
    "    for i, row in stats5.iterrows():\n",
    "        cat_index = np.where(df_rank5['Label'].unique() == row['Label'])[0][0]\n",
    "        axs[1].axhline(y=row['mean'], color='red', linestyle='-', linewidth=3, xmin=cat_index/len(df_rank5['Label'].unique()), xmax=(cat_index+1)/len(df_rank5['Label'].unique()))\n",
    "\n",
    "    axs[0].set_ylim((0,1))\n",
    "    axs[0].set(xlabel=None)\n",
    "    axs[0].set_title('Model performance: top ranked assignment')\n",
    "    axs[0].set_yticks([0,0.25,0.5,0.75,1.0])\n",
    "\n",
    "    axs[1].set_ylim((0,1))\n",
    "    axs[1].set(xlabel=None)\n",
    "    axs[1].set_title('Model performance: top 5 assignments')\n",
    "    axs[1].set_yticks([0,0.25,0.5,0.75,1.0])\n",
    "    axs[1].set_xticklabels(['', 'Base', 'Retrained', 'Base', 'Retrained'])\n",
    "\n",
    "    axs[0].spines[['right', 'top']].set_visible (False)\n",
    "    axs[0].axhline(1.0, ls='--', c='grey')\n",
    "    axs[0].axhline(0.75, ls='--', c='grey')\n",
    "    axs[0].axhline(0.5, ls='--', c='grey')\n",
    "    axs[0].axhline(0.25, ls='--', c='grey')\n",
    "\n",
    "    axs[1].spines[['right', 'top']].set_visible (False)\n",
    "    axs[1].axhline(1.0, ls='--', c='grey')\n",
    "    axs[1].axhline(0.75, ls='--', c='grey')\n",
    "    axs[1].axhline(0.5, ls='--', c='grey')\n",
    "    axs[1].axhline(0.25, ls='--', c='grey')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df_dataset\n",
    "\n",
    "df_dataset = gen_plots_benchmark(accs_full_CPD, accs_NP_unmatch, accs_full, accs_NP_unmatch_CRF, accs_full_CRF, accs_pretrain_fDNC, dataset_dict, skipfiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_table(df_dataset):\n",
    "    acc_table = pd.DataFrame(columns=['Worm', 'Dataset', 'CPD', 'fDNC', 'StatAtlas (base)', 'StatAtlas (retrain)', 'CRF (base)', 'CRF (retrain)'])\n",
    "    for file in df_dataset['Filename'].unique():\n",
    "        df_file = df_dataset[(df_dataset['Filename']==file)&(df_dataset['Rank']==1)]\n",
    "        dataset= df_file.iloc[0]['Dataset']\n",
    "        CPD = df_file[df_file['Label']=='CPD (best template)'].iloc[0]['Accuracy']\n",
    "        Stat_base = df_file[df_file['Label']=='StatAtlas (base)'].iloc[0]['Accuracy']\n",
    "        Stat_full = df_file[df_file['Label']=='StatAtlas (retrain)'].iloc[0]['Accuracy']\n",
    "        CRF_base = df_file[df_file['Label']=='CRF (base)'].iloc[0]['Accuracy']\n",
    "        CRF_full = df_file[df_file['Label']=='CRF (retrain)'].iloc[0]['Accuracy']\n",
    "        fDNC_base = df_file[df_file['Label']=='fDNC (base)'].iloc[0]['Accuracy']\n",
    "\n",
    "        acc_table.loc[len(acc_table.index)] = [file, dataset, CPD, fDNC_base, Stat_base, Stat_full, CRF_base, CRF_full]\n",
    "\n",
    "    return acc_table\n",
    "\n",
    "acc_table = get_acc_table(df_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_table = acc_table.sort_values(['Dataset','Worm'])\n",
    "acc_table.to_csv('Data/summary_acc_ID.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.histplot(data= df_dataset[(df_dataset['Rank']==1) & (df_dataset['Label']=='CPD (best template)')], x='Num_labeled', bins=16)\n",
    "axs.set_xlabel('Number of ground truth labels')\n",
    "axs.spines[['right', 'top']].set_visible (False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "num_labels = np.asarray(df_dataset[(df_dataset['Rank']==1) & (df_dataset['Label']=='CPD (best template)')]['Num_labeled'])\n",
    "\n",
    "print(sum(num_labels>=100))\n",
    "print(sum(num_labels<100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NWB-dev",
   "language": "python",
   "name": "nwb-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
